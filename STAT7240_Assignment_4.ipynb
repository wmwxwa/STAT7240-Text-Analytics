{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-Qn65e6cPUR"
      },
      "source": [
        "In this assignment - we're going to use a new dataset of tweets with sentiment labels to train a neural network based sentiment analyzer\n",
        "\n",
        "---\n",
        "\n",
        "*Fill in the missing code from the relevant sections below, missing code is indicated by \\<FILL_CODE>*\n",
        "\n",
        "*IMPORTANT: Make sure you include the outputs or printouts for every cell in the .ipynb file that you upload.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6baSjCH3iQ7t"
      },
      "source": [
        "## Import & Pre-process data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeO3gcvniWy7"
      },
      "source": [
        "### 1. Load the data, preprocess & tokenize as appropriate for tweets (2 points)\n",
        "\n",
        "* Load data from the twitter_sentiment_train_150k.txt file into a dataframe with columns - 'label' and 'tweet'. \n",
        "* Put preprocessed text into column named 'tweet_cleaned' and tokens into 'tweet_tokens'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88_oFb2LvwG-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "twitter_training = pd.read_csv('./twitter_sentiment_train_150k.txt', sep='\\t', header=None, names=['label', 'tweet'])\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    tweet = re.sub(r'[^a-zA-Z]', ' ', tweet)\n",
        "    tweet_tokens = word_tokenize(tweet)\n",
        "    tweet_tokens = [word for word in tweet_tokens if word not in stop_words]\n",
        "    tweet_tokens = [lemmatizer.lemmatize(word) for word in tweet_tokens]\n",
        "    return tweet_tokens\n",
        "\n",
        "twitter_training['tweet_cleaned'] = twitter_training['tweet'].apply(preprocess_tweet)\n",
        "twitter_training['tweet_tokens'] = twitter_training['tweet_cleaned'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "print(twitter_training['tweet_tokens'].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4UixEZ9p_qY"
      },
      "source": [
        "### 2. Load & split your testing data into validation (1/3) & testing datasets (2/3rd) - (1 point)\n",
        "\n",
        "* Similarly load test data from the twitter_sentiment_test_63k.txt file\n",
        "* Pre-process and tokenize the testing data as well\n",
        "* Split testing data into testing and validation data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axCT8BJ41OYa"
      },
      "outputs": [],
      "source": [
        "\n",
        "twitter_t_and_v = pd.read_csv('./twitter_sentiment_test_62k.txt', sep='\\t', header=None, names=['label', 'tweet'])\n",
        "\n",
        "twitter_t_and_v['tweet_cleaned'] = twitter_t_and_v['tweet'].apply(preprocess_tweet)\n",
        "twitter_t_and_v['tweet_tokens'] = twitter_t_and_v['tweet_cleaned'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "\n",
        "test_data = twitter_t_and_v.sample(frac=0.6667, random_state=0)\n",
        "validation_data = twitter_t_and_v.drop(test_data.index)\n",
        "\n",
        "print(test_data.shape)\n",
        "print(validation_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Dd3vc1kfsl"
      },
      "source": [
        "## Vectorize, setup model & train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oHNRft-jbix"
      },
      "source": [
        "### 3. Vectorize using a count vectorizer (1 point)\n",
        "\n",
        "Use the Tokenizer and text_to_word_sequence functions from keras to convert the text in the 'tweet_cleaned' column from all 3 datasets into tokens and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGlAm-fo2PZF"
      },
      "outputs": [],
      "source": [
        "MAX_FEATURES = 5000\n",
        "MAX_LENGTH = 100\n",
        "\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
        "tokenizer.fit_on_texts(twitter_training['tweet_tokens'])\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(twitter_training['tweet_tokens'])\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['tweet_tokens'])\n",
        "val_sequences = tokenizer.texts_to_sequences(validation_data['tweet_tokens'])\n",
        "\n",
        "train_texts = pad_sequences(train_sequences, maxlen=MAX_LENGTH)\n",
        "test_texts = pad_sequences(test_sequences, maxlen=MAX_LENGTH)\n",
        "val_texts = pad_sequences(val_sequences, maxlen=MAX_LENGTH)\n",
        "\n",
        "train_labels = twitter_training['label']\n",
        "test_labels = test_data['label']\n",
        "val_labels = validation_data['label']\n",
        "\n",
        "print(len(train_texts))\n",
        "print(len(train_labels))\n",
        "\n",
        "print(len(test_texts))\n",
        "print(len(test_labels))\n",
        "\n",
        "print(len(val_texts))\n",
        "print(len(val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbghUsN72KM3"
      },
      "outputs": [],
      "source": [
        "from keras.utils import pad_sequences\n",
        "\n",
        "MAX_LENGTH = max(len(train_ex) for train_ex in train_texts)\n",
        "train_texts = pad_sequences(train_texts, maxlen=MAX_LENGTH)\n",
        "test_texts = pad_sequences(test_texts, maxlen=MAX_LENGTH)\n",
        "validation_texts = pad_sequences(val_texts, maxlen=MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Xg0wH_MU6k"
      },
      "source": [
        "### 4. Why do you have to pad sequences? Explain (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ehmtnuZMadm"
      },
      "source": [
        "Answer: The neural network needs all inputs to be the same length, so you have to pad sequences to make all seuqneces the same length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByrhdaKSjhxl"
      },
      "source": [
        "### We're going to use a neural network with the following architecture for this part of the assignment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImdbGUu418OX"
      },
      "outputs": [],
      "source": [
        "from keras import layers, Input, Model\n",
        "\n",
        "sequences = Input(shape=(MAX_LENGTH,))\n",
        "embedded = layers.Embedding(MAX_FEATURES, 64)(sequences)\n",
        "x = layers.SimpleRNN(128, return_sequences=True)(embedded)\n",
        "x = layers.SimpleRNN(128)(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "x = layers.Dense(100, activation='relu')(x)\n",
        "predictions = layers.Dense(1, activation='sigmoid')(x)\n",
        "rnn_model = Model(inputs=sequences, outputs=predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1qF4AK9j62u"
      },
      "source": [
        "### 6. Train your model with the following settings (1 point)\n",
        "\n",
        "Note: The actual performance of your model is less important for this assignment. So if your model takes a while to train - you have some options to shorten the training time:\n",
        "*   Use a GPU to accelerate training (colab has these as well)\n",
        "*   Decrease the number of epochs (minimum 5 for this assignment)\n",
        "*   Decrease the size of your training data through random sampling\n",
        "*   If you have a different methodology that you'd like to try - as long as you can justify using it (provide the justification as well)- feel free to use and submit that as well\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN_AXHbt10NB"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "# first compile the model using the correct loss function & metrics\n",
        "rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# then save the history of the model training\n",
        "rnn_history = rnn_model.fit(train_texts, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(validation_texts, val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3Ge5sFwFYmv"
      },
      "outputs": [],
      "source": [
        "history_dict = rnn_history.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "epochs_range = range(1, epochs+1)\n",
        "print(list(epochs_range))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G-73IxHhbKW"
      },
      "source": [
        "### Plot accuracy vs val_accuracy below "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb5187cWFyhr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs_range, acc, 'bo', label='Training acuracy')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs_range, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_NfCixPLXg9"
      },
      "source": [
        "### 7. What is the difference between accuracy & val_accuracy? How should you change your training strategy (if at all) based on the differences between accuracy & val_accuracy? (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCqSQ1M7xqop"
      },
      "source": [
        "Answer: Accuracy is the accuracy of the model on the training data and val_accuracy is the accuracy of the model on the validation data. If the val_accuracy is much higher than the accuracy, then the model is overfitting the training data. To fix this, you can increase the number of epochs, increase the size of the training data, or decrease the size of the validation data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSZd_KSd565_"
      },
      "source": [
        "### Plot loss vs val_loss below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5NmKWizwfWd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs_range, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs_range, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASO22wNjLqHU"
      },
      "source": [
        "### 8. What is the difference between loss & val_loss and why do they matter? (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v-A9EVcm2hB"
      },
      "source": [
        "Answer: The loss of the model on the training data vs the validation data. They matter because if the loss is much higher than the val_loss, then the model is underfitting the training data. If the val_loss is much higher than the loss, then the model is overfitting the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS2M792FkAmf"
      },
      "source": [
        "### 9. Evaluate your model on your test dataset. How are the loss & accuracy here different from the earlier losses & accuracies? (1 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS5txcE4E8t5"
      },
      "outputs": [],
      "source": [
        "rnn_model.evaluate(test_texts, test_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG6xQ-r5km6d"
      },
      "source": [
        "## Re-train model using the embeddings provided"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X5ThX8H7wg9"
      },
      "source": [
        "### Convert your text data into embeddings data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unQVa6phAopV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "embeddings_index = {}\n",
        "\n",
        "# load embeddings provided\n",
        "#with open(\"/content/drive/MyDrive/Wharton/Models/glove.6B.100d.txt\") as f:\n",
        "with open(\"./glove.6B.100d.txt\", errors=\"ignore\") as f:\n",
        "  for line in f:\n",
        "    word, coefs = line.split(maxsplit=1)\n",
        "    coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "    embeddings_index[word] = coefs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_vvWGQT7auu"
      },
      "outputs": [],
      "source": [
        "# vectorize again before creating the embeddings\n",
        "import tensorflow as tf\n",
        "from keras.layers import TextVectorization\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=MAX_FEATURES, output_sequence_length=MAX_LENGTH)\n",
        "tweets_as_strings = twitter_training['tweet_cleaned'].apply(' '.join).values\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(tweets_as_strings).batch(128)\n",
        "vectorizer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPjmTzRbAn9V"
      },
      "outputs": [],
      "source": [
        "# Extract vocabuary & word index\n",
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))\n",
        "\n",
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ctDOYfzkO_I"
      },
      "source": [
        "### 10. Use an embedding layer as the input layer instead of counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A9z8SB-75LS"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Embedding\n",
        "from keras.initializers import Constant\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37XHvzCEyq1_"
      },
      "outputs": [],
      "source": [
        "# specify input layer\n",
        "rnn_inputs = Input(shape=(MAX_LENGTH,), dtype=\"int64\")\n",
        "rnn_embedded_sequences = embedding_layer(rnn_inputs)\n",
        "x = layers.CuDNNGRU(128, return_sequences=True)(rnn_embedded_sequences)\n",
        "x = layers.CuDNNGRU(128)(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "x = layers.Dense(100, activation='relu')(x)\n",
        "predictions = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "rnn_model_embeddings = Model(inputs=rnn_inputs, outputs=predictions)\n",
        "rnn_model_embeddings.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfv1aV4bfd30"
      },
      "source": [
        "### 11. In creating the embedding layer in the previous section of code - what does the embedding layer do and what are the first 2 parameters? (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK7VO6eJfyt_"
      },
      "source": [
        "Answer: The embedding layer converts the input data into embeddings. The first parameter is the number of unique tokens and the second parameter is the dimension of the embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDU9Z5Nkf8Pp"
      },
      "source": [
        "### Compile & Train your model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiU9XyGsf8Pq"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "# first compile the model using the correct loss function & metrics\n",
        "rnn_model_embeddings.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# then save the history of the model training\n",
        "rnn_embeddings_history = rnn_model.fit(train_texts, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(validation_texts, val_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtzgglNpkYBL"
      },
      "source": [
        "### 12. Evaluate your new model (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRNDr0MSzc23"
      },
      "outputs": [],
      "source": [
        "rnn_model_embeddings.evaluate(test_texts, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myLV0LWUkeV_"
      },
      "source": [
        "### 13. Why did you model perform better or worse after switching to an embedding input layer? Explain... (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zifKcwRCkpfz"
      },
      "source": [
        "Answer: Embeddings are more accurate than the counts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2Zrsr3B1_L7"
      },
      "source": [
        "### 14. Did the model train faster or slower with embeddings or without using embeddings as the inputs? Why was it faster or slower? (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fofzE_jL2OR9"
      },
      "source": [
        "Answer: Faster with embeddings due to decreased complexity of the model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "14c2485957b2ce2426b726e732a99c51660700eeffcabbe66967fd94d549347c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
