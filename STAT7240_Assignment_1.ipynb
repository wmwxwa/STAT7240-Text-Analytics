{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pys0RUR6ZR5E"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiaIAX5xZUta"
      },
      "source": [
        "## a. Load the book by Victor Hugo from the URL provided or your own, then using NLTK - compute the number of sentences & words in the book? (2 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j9QmhE9fZMHG"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import nltk\n",
        "\n",
        "url = 'https://www.gutenberg.org/files/135/135-0.txt'\n",
        "\n",
        "# load text data from url 'https://www.gutenberg.org/files/135/135-0.txt'\n",
        "\n",
        "text = requests.get(url).text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xnFdjRO_aBGn"
      },
      "outputs": [],
      "source": [
        "# tokenize the raw text into words and sentences\n",
        "from nltk.tokenize import word_tokenize\n",
        "words = word_tokenize(text)\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iTLQvafLbMcH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "664358\n",
            "29647\n"
          ]
        }
      ],
      "source": [
        "# print their lengths\n",
        "print(len(words))\n",
        "print(len(sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSMpajEmevgJ"
      },
      "source": [
        "## b. Remove all punctuation/symbols and convert all text to lowercase. Then re-count words and sentences (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MoTNme46d9vn"
      },
      "outputs": [],
      "source": [
        "# remove all punctuation and convert all text to lowercase\n",
        "import re\n",
        "\n",
        "def remove_punctuations(text):\n",
        "  return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "text_cleaned = remove_punctuations(text)\n",
        "\n",
        "text_cleaned_lower = text_cleaned.lower()\n",
        "\n",
        "# tokenize the cleaned text into words and sentences\n",
        "words_cleaned = word_tokenize(text_cleaned_lower)\n",
        "sentences_cleaned = sent_tokenize(text_cleaned_lower)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "En-1I9OYe7uw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "568217\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "#print their lengths (change the variable names as needed)\n",
        "words_cleaned = word_tokenize(text_cleaned_lower)\n",
        "sentences_cleaned = sent_tokenize(text_cleaned_lower)\n",
        "\n",
        "\n",
        "print(len(words_cleaned))\n",
        "print(len(sentences_cleaned))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_5a1xVqalQx"
      },
      "source": [
        "## c. Print the 20 most frequent words in your raw text? (1 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qj3cWTVbcAmP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(',', 48757), ('the', 36466), ('.', 26169), ('of', 19590), ('and', 14005), ('a', 13377), ('to', 13295), ('in', 10231), ('was', 8535), ('that', 7181)]\n"
          ]
        }
      ],
      "source": [
        "#Print the 20 most frequent words in your raw text?\n",
        "from nltk.probability import FreqDist\n",
        "print(nltk.FreqDist(words).most_common(10))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm5mfpVRaq9A"
      },
      "source": [
        "## d. After removing stop words as defined by NLTK, what are the 20 most frequent words in your corpus? (1 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JmFA_xhVckxO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\wangwilliam\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ïthe', 'project', 'gutenberg', 'ebook', 'les', 'misãrables', 'victor', 'hugo', 'ebook', 'use', 'anyone', 'anywhere', 'united', 'states', 'parts', 'world', 'cost', 'almost', 'restrictions', 'whatsoever']\n"
          ]
        }
      ],
      "source": [
        "#After removing stop words as defined by NLTK, what are the 20 most frequent words in your corpus?\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "def remove_stopwords(text_list):\n",
        "  return [words_cleaned for words_cleaned in text_list if words_cleaned not in stopwords]\n",
        "\n",
        "text_tokens_nostops = remove_stopwords(words_cleaned)\n",
        "print(text_tokens_nostops[:20])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2r0cEDICXY3"
      },
      "source": [
        "## e. Add custom stopwords, then print the 20 most frequent words after removing your custom stopwords (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7GEUwOmPCvB2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['may', 'copy', 'give', 'away', 'reuse', 'terms', 'license', 'included', 'online', 'wwwgutenbergorg', 'located', 'check', 'laws', 'country', 'located', 'using', 'title', 'complete', 'five', 'volumes']\n"
          ]
        }
      ],
      "source": [
        "stopwords.extend(['ïthe', 'project', 'gutenberg', 'ebook', 'les', 'misãrables', 'victor', 'hugo', 'ebook', 'use', 'anyone', 'anywhere', 'united', 'states', 'parts', 'world', 'cost', 'almost', 'restrictions', 'whatsoever'])\n",
        "text_tokens_no_custom_stops = remove_stopwords(words_cleaned)\n",
        "print(text_tokens_no_custom_stops[:20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6dkYdRya1bS"
      },
      "source": [
        "## f. After lemmatization using NLTK's WordNetLemmatizer - what are the 20 most frequent words in your corpus? (1 point)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AWknYEevghAt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\wangwilliam\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\wangwilliam\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['may', 'copy', 'give', 'away', 'reuse', 'term', 'license', 'included', 'online', 'wwwgutenbergorg', 'located', 'check', 'law', 'country', 'located', 'using', 'title', 'complete', 'five', 'volume']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmer = WordNetLemmatizer()\n",
        "\n",
        "def lemma_words(token_list):\n",
        "  return [lemmer.lemmatize(w) for w in token_list]\n",
        "\n",
        "text_tokens_lemmad = lemma_words(text_tokens_no_custom_stops)\n",
        "\n",
        "print(text_tokens_lemmad[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeCjwZwNl0Au"
      },
      "source": [
        "## g. Looking at the most frequent words before and after various levels of text pre-processing - why is pre-processing necessary. Are there situations where it might not be needed? (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et64omefpwr7"
      },
      "source": [
        "Pre-preocessing removes content that does not provide much value. It might not be needed when the content is already a list or something similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOFdCd9WhjHM"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LcHo3r5MTiV"
      },
      "source": [
        "## a. Load the 'Planet Money' podcast xml from the url below and extract the titles & descriptions into a list (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SqmsdtZJMkws"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'\n",
            " '<rss xmlns:npr=\"https://www.npr.org/rss/\" '\n",
            " 'xmlns:nprml=\"https://api.npr.org/nprml\" '\n",
            " 'xmlns:itunes=\"http://www.itunes.com/dtds/podcast-1.0.dtd\" '\n",
            " 'xmlns:content=\"http://purl.org/rss/1.0/modules/content/\" '\n",
            " 'xmlns:dc=\"http://purl.org/dc/elements/1.1/\" '\n",
            " 'xmlns:media=\"http://search.yahoo.com/mrss/\" version=\"2.0\">\\n'\n",
            " '  <channel>\\n'\n",
            " '    <title>Planet Money</title>\\n'\n",
            " '    <link>https://www.npr.org/podcasts/510289/planet-money</link>\\n'\n",
            " '    <description><![CDATA[Wanna see a trick? Give u')\n",
            "355\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<item>\n",
              "<title>Two Indicators shaking China's economy</title>\n",
              "<description>Xi Jinping recently secured his third term as China's president – so we're looking at two shocks to the world's second-largest economy. First: How China's housing boom turned into a real estate crisis. Second: How the recent U.S. ban on selling advanced semiconductor chips to China could affect China's technology industry.  </description>\n",
              "<pubDate>Wed, 02 Nov 2022 18:58:55 -0400</pubDate>\n",
              "<copyright>Copyright 2015-2021 NPR - For Personal Use Only</copyright>\n",
              "<guid isPermalink=\"false\">65710e95-84ed-4fb3-b6e6-49f38bd76b8d</guid>\n",
              "<link>https://www.npr.org/2022/11/02/1133703176/two-indicators-shaking-chinas-economy</link>\n",
              "<itunes:title>Two Indicators shaking China's economy</itunes:title>\n",
              "<itunes:episode>1587</itunes:episode>\n",
              "<itunes:author>NPR</itunes:author>\n",
              "<itunes:summary>Why China's housing market went from boom to bust, and how the new ban on selling high-end semiconductor chips to China may affect its tech sector</itunes:summary>\n",
              "<itunes:subtitle>Why China's housing market went from boom to bust, and how the new ban on selling high-end semiconductor chips to China may affect its tech sector</itunes:subtitle>\n",
              "<itunes:image href=\"https://media.npr.org/assets/img/2022/11/02/gettyimages-1241972535_wide-23b3e10b2c7b1a073106378072e47bec3e11dd63.jpg?s=1400\"/>\n",
              "<itunes:duration>1165</itunes:duration>\n",
              "<itunes:explicit>no</itunes:explicit>\n",
              "<itunes:episodeType>full</itunes:episodeType>\n",
              "<content:encoded>Xi Jinping recently secured his third term as China's president – so we're looking at two shocks to the world's second-largest economy. First: How China's housing boom turned into a real estate crisis. Second: How the recent U.S. ban on selling advanced semiconductor chips to China could affect China's technology industry.  </content:encoded>\n",
              "<enclosure length=\"18645204\" type=\"audio/mpeg\" url=\"https://play.podtrac.com/npr-510289/edge1.pod.npr.org/anon.npr-mp3/npr/pmoney/2022/11/20221102_pmoney_649608c7-874d-4e3b-9449-d1eaa7c24a85.mp3?awCollectionId=510289&amp;awEpisodeId=1133703176&amp;orgId=1&amp;topicId=1006&amp;d=1165&amp;p=510289&amp;story=1133703176&amp;t=podcast&amp;e=1133703176&amp;size=18645204&amp;ft=pod&amp;f=510289\"/>\n",
              "</item>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from pprint import pprint\n",
        "import requests\n",
        "uri = \"https://feeds.npr.org/510289/podcast.xml\"\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
        "\n",
        "xml = requests.get(uri, headers=headers).content.decode()\n",
        "pprint(xml[:500])\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "xml_soup = BeautifulSoup(xml, 'xml')\n",
        "all_items = xml_soup.find_all('item')\n",
        "print(len(all_items))\n",
        "all_items[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BWP4EgDCMxac"
      },
      "outputs": [],
      "source": [
        "# put all the titles and descriptions into 2 lists - all_titles & all_descriptions\n",
        "\n",
        "all_titles = []\n",
        "for title in xml_soup.find_all('title'):\n",
        "  all_titles.append(title.get_text())\n",
        "\n",
        "all_descriptions = []\n",
        "for description in xml_soup.find_all('description'):\n",
        "  all_descriptions.append(description.get_text())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b643vu9fO0um"
      },
      "source": [
        "## b. First pre-process the text from the <description> tag and convert the text into a list of tokens. Then print the 10 most frequent words (2 points)\n",
        "\n",
        "While pre-processing - also remove html elements (`<tag>`) from the descriptions using  `text = re.sub(r'<.*?>', '', text)` while pre-processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PPQ_n8kWQHTY"
      },
      "outputs": [],
      "source": [
        "\n",
        "#nltk.download('stopwords')\n",
        "#from nltk.corpus import stopwords\n",
        "#stopwords = stopwords.words('english')\n",
        "\n",
        "#def remove_stopwords(text_list):\n",
        "#  return [word for word in text_list if word not in stopwords]\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "# list containing all the description tokens\n",
        "\n",
        "all_descriptions_tokens = []\n",
        "\n",
        "def pre_process(text):\n",
        "  # convert text in list to lowercase\n",
        "  text = text.lower()  \n",
        "  # remove html tags & remove symbols\n",
        "  text = re.sub(r'<.*?>', '', str(text))\n",
        "  text = re.sub(r'[^\\w\\s]', '', str(text))\n",
        "  # tokenize into words\n",
        "  tokens = word_tokenize(text)\n",
        "  # remove stopwords\n",
        "  tokens = remove_stopwords(tokens)\n",
        "  return tokens\n",
        "\n",
        "for text in all_descriptions:\n",
        "  all_descriptions_tokens.extend(pre_process(text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Mj-_v3j-eATS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('subscribe', 288), ('newsletter', 267), ('weekly', 265), ('money', 86), ('planet', 64), ('us', 55), ('one', 49), ('economy', 44), ('show', 38), ('people', 35)]\n"
          ]
        }
      ],
      "source": [
        "# print the most frequent words in all_descriptions_tokens\n",
        "from nltk.probability import FreqDist\n",
        "print(nltk.FreqDist(all_descriptions_tokens).most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n15Ov-L6sYkn"
      },
      "source": [
        "## c. Based on the list of most frequent words from the descriptions printed above - assess which words may carry no informational value and are common across all the descriptions? Remove them, then print the 10 most frequent words again (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "43TM6o7ufK-A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('money', 86), ('planet', 64), ('us', 55), ('one', 49), ('economy', 44), ('show', 38), ('people', 35), ('apple', 34), ('two', 33), ('plusnprorgplanetmoney', 32)]\n"
          ]
        }
      ],
      "source": [
        "stopwords.extend(['subscribe', 'newsletter', 'weekly'])\n",
        "all_descriptions_tokens_nostops = remove_stopwords(all_descriptions_tokens)\n",
        "print(nltk.FreqDist(all_descriptions_tokens_nostops).most_common(10))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "14c2485957b2ce2426b726e732a99c51660700eeffcabbe66967fd94d549347c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
